{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import densenet121\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LANGKAH 1: MODIFIKASI MODEL UNTUK MULTI-CLASS (CheXNet/DenseNet121)\n",
    "# =============================================================================\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengatur random seed agar hasil eksperimen reproducible.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # jika menggunakan multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Panggil fungsi ini sekali di awal skrip Anda\n",
    "SEED = 42 # Angka 42 adalah konvensi, bisa diganti angka lain\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Random seed diatur ke {SEED}\")\n",
    "\n",
    "class CheXNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model CheXNet berbasis DenseNet121 untuk klasifikasi multi-class.\n",
    "    \"\"\"\n",
    "    def _init_(self, num_classes, pretrained=True, checkpoint_path=None):\n",
    "        super(CheXNetModel, self)._init_()\n",
    "        \n",
    "        # Load DenseNet121 sebagai backbone\n",
    "        self.densenet = densenet121(pretrained=pretrained)\n",
    "        \n",
    "        # Ganti classifier untuk multi-class\n",
    "        num_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Load pre-trained weights jika tersedia\n",
    "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "            self.load_pretrained_weights(checkpoint_path, num_classes)\n",
    "    \n",
    "    def load_pretrained_weights(self, checkpoint_path, num_classes):\n",
    "        \"\"\"\n",
    "        Load pre-trained CheXNet weights dan adaptasi untuk jumlah kelas yang berbeda.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Coba load checkpoint\n",
    "            if os.path.isdir(checkpoint_path):\n",
    "                # Cari file .pth atau .pt dalam direktori\n",
    "                for file in os.listdir(checkpoint_path):\n",
    "                    if file.endswith(('.pth', '.pt')):\n",
    "                        checkpoint_path = os.path.join(checkpoint_path, file)\n",
    "                        break\n",
    "            \n",
    "            if checkpoint_path.endswith(('.pth', '.pt')):\n",
    "                checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "                \n",
    "                # Handle different checkpoint formats\n",
    "                if 'state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['state_dict']\n",
    "                elif 'model' in checkpoint:\n",
    "                    state_dict = checkpoint['model']\n",
    "                else:\n",
    "                    state_dict = checkpoint\n",
    "                \n",
    "                # Remove 'module.' prefix if present (from DataParallel)\n",
    "                new_state_dict = {}\n",
    "                for k, v in state_dict.items():\n",
    "                    name = k[7:] if k.startswith('module.') else k\n",
    "                    new_state_dict[name] = v\n",
    "                \n",
    "                # Load weights, excluding final classifier if different num_classes\n",
    "                model_dict = self.densenet.state_dict()\n",
    "                pretrained_dict = {k: v for k, v in new_state_dict.items() \n",
    "                                 if k in model_dict and 'classifier' not in k}\n",
    "                \n",
    "                model_dict.update(pretrained_dict)\n",
    "                self.densenet.load_state_dict(model_dict, strict=False)\n",
    "                print(f\"Pre-trained weights loaded from {checkpoint_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not load pre-trained weights: {e}\")\n",
    "            print(\"Using ImageNet pre-trained weights instead.\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.densenet(x)\n",
    "\n",
    "# =============================================================================\n",
    "# LANGKAH 2: DATASET CUSTOM UNTUK PYTORCH\n",
    "# =============================================================================\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def _init_(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            # Bagian untuk train/val (TIDAK BERUBAH)\n",
    "            self.dataset = datasets.ImageFolder(root_dir)\n",
    "            self.samples = self.dataset.samples\n",
    "            self.classes = self.dataset.classes\n",
    "        else:\n",
    "            # Bagian untuk test\n",
    "            self.image_paths = []\n",
    "            for file in sorted(os.listdir(root_dir)):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(root_dir, file))\n",
    "    \n",
    "    def _len_(self):\n",
    "        if self.is_test:\n",
    "            return len(self.image_paths)\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _getitem_(self, idx):\n",
    "        if self.is_test:\n",
    "            # PASTIKAN BAGIAN INI BENAR: HANYA MENGEMBALIKAN PATH\n",
    "            img_path = self.image_paths[idx]\n",
    "            return img_path, os.path.basename(img_path) # <-- PERUBAHAN KUNCI\n",
    "        else:\n",
    "            # Bagian untuk train/val (TIDAK BERUBAH)\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def _init_(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self)._init_()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "# =============================================================================\n",
    "# LANGKAH 3: KONFIGURASI DAN PERSIAPAN DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Atur parameter utama\n",
    "IMG_SIZE = (384, 384)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Path ke direktori data\n",
    "TRAIN_PATH = \"/kaggle/input/srifoton-25-machine-learning-competition/train/train\"\n",
    "VAL_PATH = \"/kaggle/input/srifoton-25-machine-learning-competition/val/val\"\n",
    "TEST_PATH = \"/kaggle/input/srifoton-25-machine-learning-competition/test\"\n",
    "CHEXNET_WEIGHTS = '/kaggle/input/chexnet/pytorch/default/1'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Buat custom transform untuk CLAHE\n",
    "class ApplyCLAHE(object):\n",
    "    def _init_(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "\n",
    "    def _call_(self, img):\n",
    "        # Konversi PIL Image ke array numpy\n",
    "        img_np = np.array(img)\n",
    "        # Jika gambar berwarna, terapkan CLAHE pada channel L dari L*a*b* space\n",
    "        if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n",
    "            lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            l_clahe = self.clahe.apply(l)\n",
    "            lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "            img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "            return Image.fromarray(img_clahe)\n",
    "        # Jika grayscale, terapkan langsung (walaupun kode Anda .convert('RGB'))\n",
    "        else:\n",
    "            gray_clahe = self.clahe.apply(img_np)\n",
    "            return Image.fromarray(gray_clahe)\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(), # Gambar diubah menjadi Tensor\n",
    "    # --- BARIS BARU DITAMBAHKAN DI SINI ---\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "    # ------------------------------------\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]) # Normalisasi setelahnya\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0), # <-- TERAPKAN JUGA PADA VALIDASI & TEST\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = ImageDataset(TRAIN_PATH, transform=train_transform)\n",
    "val_dataset = ImageDataset(VAL_PATH, transform=val_test_transform)\n",
    "test_dataset = ImageDataset(TEST_PATH, transform=val_test_transform, is_test=True)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes found: {class_names}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                         shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                       shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LANGKAH 4: MEMBUAT DAN MELATIH MODEL\n",
    "# =============================================================================\n",
    "model = CheXNetModel(num_classes=num_classes, pretrained=True, checkpoint_path=CHEXNET_WEIGHTS)\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "# Loss function (sama seperti sebelumnya)\n",
    "criterion = FocalLoss(gamma=2)\n",
    "\n",
    "# --- TAHAP 1: PEMANASAN (TRAIN CLASSIFIER HEAD SAJA) ---\n",
    "print(\"\\n--- Starting Stage 1: Training Classifier Head ---\")\n",
    "\n",
    "# Bekukan semua lapisan kecuali classifier\n",
    "for param in model.densenet.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Buat optimizer yang HANYA menargetkan classifier head\n",
    "optimizer_head = optim.Adam(model.densenet.classifier.parameters(), lr=LEARNING_RATE)\n",
    "epochs_head = 5  # Latih head selama 5 epoch\n",
    "\n",
    "for epoch in range(epochs_head):\n",
    "    print(f\"\\nHead Training Epoch [{epoch+1}/{epochs_head}]\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_head, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Head Train Loss: {train_loss:.4f}, Head Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Head Val Loss: {val_loss:.4f}, Head Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "# --- TAHAP 2: PENYEMPURNAAN (TRAIN SELURUH MODEL) ---\n",
    "print(\"\\n--- Starting Stage 2: Fine-tuning Full Model ---\")\n",
    "\n",
    "# Cairkan kembali seluruh lapisan model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Buat optimizer baru untuk seluruh model dengan LEARNING RATE LEBIH KECIL\n",
    "optimizer_full = optim.AdamW(model.parameters(), lr=LEARNING_RATE / 10, weight_decay=1e-4) # Coba AdamW!\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_full, mode='max', factor=0.5, patience=3, verbose=True) # Mode 'max' untuk akurasi\n",
    "\n",
    "# Training loop utama Anda (sedikit dimodifikasi)\n",
    "# Gunakan optimizer_full dan scheduler yang baru\n",
    "print(\"\\nStarting full model training...\")\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "patience = 7 # Mungkin perlu sedikit lebih sabar\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS}]\")\n",
    "    \n",
    "    # Gunakan optimizer_full\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer_full, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Scheduler sekarang memantau akurasi validasi\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Early stopping (sama seperti sebelumnya)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        patience_counter = 0\n",
    "        print(f\"New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "print(f\"Training completed. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# LANGKAH 5: PLOT TRAINING HISTORY\n",
    "# =============================================================================\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs_range = range(1, len(train_accs) + 1)\n",
    "plt.plot(epochs_range, train_accs, label='Training Accuracy', marker='o')\n",
    "plt.plot(epochs_range, val_accs, label='Validation Accuracy', marker='s')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# =============================================================================\n",
    "# BAGIAN 2: PROSES PREDIKSI\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Memulai proses prediksi...\")\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformasi dengan flip horizontal\n",
    "hflip_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.RandomHorizontalFlip(p=1.0), # p=1.0 agar pasti di-flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformasi dengan sedikit rotasi\n",
    "rotate_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.RandomRotation(degrees=10), # Rotasi acak +/- 10 derajat\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Kumpulkan semua transformasi dalam sebuah list\n",
    "tta_transforms = [base_transform, hflip_transform, rotate_transform]\n",
    "print(f\"Menggunakan {len(tta_transforms)} transformasi untuk TTA.\")\n",
    "\n",
    "\n",
    "# --- Konfigurasi ---\n",
    "IMG_SIZE = (384, 384)\n",
    "BATCH_SIZE = 16\n",
    "TEST_PATH = \"/kaggle/input/srifoton-25-machine-learning-competition/test/test\" \n",
    "TRAIN_PATH = \"/kaggle/input/srifoton-25-machine-learning-competition/train/train\"\n",
    "MODEL_PATH = 'best_model.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Dapatkan Nama Kelas dari Folder Training ---\n",
    "# Ini penting untuk memastikan urutan kelas yang dipelajari model\n",
    "temp_train_dataset = datasets.ImageFolder(TRAIN_PATH)\n",
    "class_names_from_folder = temp_train_dataset.classes\n",
    "num_classes = len(class_names_from_folder)\n",
    "print(f\"Urutan kelas yang dipelajari model: {class_names_from_folder}\")\n",
    "\n",
    "# --- Siapkan Model ---\n",
    "print(f\"Memuat model dari {MODEL_PATH}...\")\n",
    "model = CheXNetModel(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Siapkan Data Test ---\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_dataset_for_tta = ImageDataset(TEST_PATH, is_test=True) \n",
    "test_loader_for_tta = DataLoader(test_dataset_for_tta, batch_size=1, shuffle=False, num_workers=0)\n",
    "print(f\"Menemukan {len(test_dataset_for_tta)} gambar untuk diprediksi dengan TTA.\")\n",
    "\n",
    "# --- Jalankan Prediksi dengan TTA ---\n",
    "print(\"\\nMembuat prediksi pada data tes menggunakan TTA...\")\n",
    "predictions_indices = []\n",
    "filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loop ini sekarang akan menerima path gambar, bukan objek gambar\n",
    "    for image_path, filename in tqdm(test_loader_for_tta, desc=\"Testing with TTA\"):\n",
    "        \n",
    "        current_filename = filename[0]\n",
    "        \n",
    "        # Buka gambar dari path di dalam loop\n",
    "        current_image = Image.open(image_path[0]).convert('RGB')\n",
    "\n",
    "        # Sisa loop TTA sama seperti sebelumnya\n",
    "        tta_probs = torch.zeros(1, num_classes).to(device)\n",
    "        \n",
    "        for tta_transform in tta_transforms:\n",
    "            transformed_image = tta_transform(current_image).unsqueeze(0).to(device)\n",
    "            outputs = model(transformed_image)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)\n",
    "            tta_probs += probs\n",
    "            \n",
    "        avg_probs = tta_probs / len(tta_transforms)\n",
    "        _, predicted = avg_probs.max(1)\n",
    "        \n",
    "        predictions_indices.append(predicted.cpu().item())\n",
    "        filenames.append(current_filename)\n",
    "\n",
    "# ... Lanjutkan ke BAGIAN 3 untuk menyimpan hasil ...\n",
    "\n",
    "class_to_number = {\n",
    "    'Bacterial Pneumonia': 0,\n",
    "    'Corona Virus Disease': 1,\n",
    "    'Normal': 2,\n",
    "    'Tuberculosis': 3,\n",
    "    'Viral Pneumonia': 4\n",
    "}\n",
    "\n",
    "# 2. Buat pemetaan dari indeks output model ke nomor yang Anda inginkan\n",
    "#    class_names_from_folder didapat dari ImageFolder (urut abjad)\n",
    "#    Contoh: Jika model memprediksi indeks 0, itu berarti 'Bacterial Pneumonia',\n",
    "#    lalu kita petakan ke nomor 0 dari class_to_number.\n",
    "final_predictions = []\n",
    "for idx in predictions_indices:\n",
    "    # Dapatkan nama kelas dari indeks prediksi model\n",
    "    class_name = class_names_from_folder[idx]\n",
    "    # Dapatkan nomor yang sesuai dari pemetaan Anda\n",
    "    number = class_to_number[class_name]\n",
    "    final_predictions.append(number)\n",
    "\n",
    "# 3. Buat DataFrame untuk submission\n",
    "results_df = pd.DataFrame({\n",
    "    'Id': filenames,\n",
    "    'Predicted': final_predictions # Gunakan prediksi numerik final\n",
    "})\n",
    "\n",
    "# 4. Simpan ke file CSV\n",
    "submission_path = 'submission.csv'\n",
    "results_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Prediksi selesai! File disimpan di: {submission_path}\")\n",
    "print(\"\\nPreview 10 prediksi pertama:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "print(f\"\\nDistribusi prediksi:\")\n",
    "# Tampilkan distribusi numerik\n",
    "print(results_df['Predicted'].value_counts().sort_index())\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Pastikan model sudah di-load dan dalam mode evaluasi\n",
    "# Jika model belum di-load, jalankan baris ini:\n",
    "# model.load_state_dict(torch.load('best_model.pth'))\n",
    "# model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Mengumpulkan prediksi dari validation set...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 1. Tampilkan Laporan Klasifikasi (Precision, Recall, F1-Score)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "\n",
    "# 2. Buat dan Tampilkan Confusion Matrix\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"=\"*50)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
