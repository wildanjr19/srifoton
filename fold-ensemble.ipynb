{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import densenet121\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FUNGSI HELPER DAN PENGATURAN SEED\n",
    "# =============================================================================\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Random seed diatur ke {SEED}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DEFINISI MODEL, DATASET, DAN FUNGSI HELPER\n",
    "# =============================================================================\n",
    "\n",
    "# --- DEFINISI MODEL CHEXNET ---\n",
    "class CheXNetModel(nn.Module):\n",
    "    def _init_(self, num_classes, pretrained=True, checkpoint_path=None):\n",
    "        super(CheXNetModel, self)._init_()\n",
    "        self.densenet = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=pretrained)\n",
    "        num_features = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "            self.load_pretrained_weights(checkpoint_path)\n",
    "\n",
    "    def load_pretrained_weights(self, checkpoint_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))\n",
    "            new_state_dict = {k[7:] if k.startswith('module.') else k: v for k, v in state_dict.items()}\n",
    "            model_dict = self.densenet.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in new_state_dict.items() if k in model_dict and 'classifier' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            self.densenet.load_state_dict(model_dict, strict=False)\n",
    "            print(f\"Pre-trained CheXNet weights loaded from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load CheXNet weights: {e}. Using ImageNet weights instead.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.densenet(x)\n",
    "\n",
    "# --- DEFINISI DATASET CUSTOM (Versi yang mendukung is_test) ---\n",
    "class CustomImageDataset(Dataset):\n",
    "    def _init_(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            self.dataset = datasets.ImageFolder(root_dir)\n",
    "            self.samples = self.dataset.samples\n",
    "            self.classes = self.dataset.classes\n",
    "        else:\n",
    "            self.image_paths = sorted([os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    \n",
    "    def _len_(self):\n",
    "        return len(self.image_paths) if self.is_test else len(self.samples)\n",
    "    \n",
    "    def _getitem_(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            filename = os.path.basename(img_path)\n",
    "            # Untuk TTA, transformasi diterapkan saat prediksi, bukan di sini\n",
    "            return image, filename\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "# --- CUSTOM TRANSFORM UNTUK CLAHE ---\n",
    "class ApplyCLAHE:\n",
    "    def _init_(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "    def _call_(self, img):\n",
    "        img_np = np.array(img)\n",
    "        if len(img_np.shape) == 3:\n",
    "            lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "            l, _, _ = cv2.split(lab)\n",
    "            l_clahe = self.clahe.apply(l)\n",
    "            lab_clahe = cv2.merge((l_clahe, cv2.split(lab)[1], cv2.split(lab)[2]))\n",
    "            return Image.fromarray(cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB))\n",
    "        return Image.fromarray(self.clahe.apply(img_np))\n",
    "\n",
    "# --- DEFINISI FOCAL LOSS ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def _init_(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self)._init_()\n",
    "        self.gamma, self.alpha, self.reduction = gamma, alpha, reduction\n",
    "        if isinstance(alpha, list): self.alpha = torch.tensor(alpha)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.device != inputs.device: self.alpha = self.alpha.to(inputs.device)\n",
    "            at = self.alpha.gather(0, targets)\n",
    "            focal_loss = at * focal_loss\n",
    "        return focal_loss.mean() if self.reduction == 'mean' else focal_loss.sum()\n",
    "\n",
    "# =============================================================================\n",
    "# 3. KONFIGURASI DAN PERSIAPAN DATA\n",
    "# =============================================================================\n",
    "IMG_SIZE = (384, 384)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "N_SPLITS = 4\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/final-lung-disease/train/train\"\n",
    "TEST_PATH = \"/kaggle/input/final-lung-disease/test/test\"\n",
    "CHEXNET_WEIGHTS = '/kaggle/input/chexnet-weights/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE), ApplyCLAHE(), transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE), ApplyCLAHE(), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset_obj = datasets.ImageFolder(TRAIN_PATH)\n",
    "class_names = full_dataset_obj.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes: {class_names} ({num_classes} classes)\")\n",
    "\n",
    "X = np.arange(len(full_dataset_obj.samples))\n",
    "y = np.array(full_dataset_obj.targets)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. FUNGSI TRAINING & VALIDATION\n",
    "# =============================================================================\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    return running_loss / len(loader.dataset), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, all_preds, all_labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return running_loss / len(loader.dataset), f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# =============================================================================\n",
    "# 5. LOOP CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "all_folds_best_f1 = []\n",
    "saved_model_paths = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n{'='*20} FOLD {fold + 1}/{N_SPLITS} {'='*20}\")\n",
    "    \n",
    "    train_subset = Subset(CustomImageDataset(TRAIN_PATH, transform=train_transform), train_idx)\n",
    "    val_subset = Subset(CustomImageDataset(TRAIN_PATH, transform=val_transform), val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    train_labels = y[train_idx]\n",
    "    class_counts = Counter(train_labels)\n",
    "    class_weights = [len(train_labels) / (num_classes * class_counts[i]) for i in range(num_classes)]\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    print(f\"Class weights for this fold: {np.round(class_weights, 2)}\")\n",
    "    \n",
    "    model = CheXNetModel(num_classes=num_classes, checkpoint_path=CHEXNET_WEIGHTS).to(device)\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=class_weights_tensor)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE / 10, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    best_fold_f1 = 0.0\n",
    "    model_save_path = f'best_model_fold_{fold+1}.pth'\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_f1 = validate_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss  : {val_loss:.4f} | Val F1  : {val_f1:.4f}\")\n",
    "        scheduler.step(val_f1)\n",
    "        if val_f1 > best_fold_f1:\n",
    "            best_fold_f1 = val_f1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"🚀 Model saved to {model_save_path} with F1: {best_fold_f1:.4f}\")\n",
    "\n",
    "    all_folds_best_f1.append(best_fold_f1)\n",
    "    saved_model_paths.append(model_save_path)\n",
    "    print(f\"\\nBest F1 for Fold {fold + 1} was: {best_fold_f1:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. HASIL CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "print(f\"\\n\\n{'='*20} HASIL CROSS-VALIDATION {'='*20}\")\n",
    "average_f1 = np.mean(all_folds_best_f1)\n",
    "print(f\"📊 Rata-rata F1-score terbaik dari {N_SPLITS}-fold CV: {average_f1:.4f}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. PREDIKSI FINAL DENGAN ENSEMBLE DARI 4-FOLD MODELS\n",
    "# =============================================================================\n",
    "print(\"\\nMemulai proses prediksi dengan TTA dan ensemble dari 4 model...\")\n",
    "\n",
    "# --- Muat semua model yang telah dilatih ---\n",
    "ensemble_models = []\n",
    "for path in saved_model_paths:\n",
    "    model = CheXNetModel(num_classes=num_classes, pretrained=False).to(device) # pretrained=False karena kita load state dict\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    ensemble_models.append(model)\n",
    "print(f\"✅ Berhasil memuat {len(ensemble_models)} model untuk ensembling.\")\n",
    "\n",
    "# --- Definisikan transformasi TTA ---\n",
    "tta_transforms = [\n",
    "    transforms.Compose([ # Base\n",
    "        transforms.Resize(IMG_SIZE), ApplyCLAHE(), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    transforms.Compose([ # Horizontal Flip\n",
    "        transforms.Resize(IMG_SIZE), ApplyCLAHE(), transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    transforms.Compose([ # Rotation\n",
    "        transforms.Resize(IMG_SIZE), ApplyCLAHE(), transforms.RandomRotation(degrees=10), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "]\n",
    "\n",
    "# --- Siapkan data test ---\n",
    "test_dataset = CustomImageDataset(TEST_PATH, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # Batch size 1 untuk TTA\n",
    "\n",
    "# --- Jalankan prediksi ---\n",
    "predictions_indices, filenames = [], []\n",
    "with torch.no_grad():\n",
    "    for image, filename in tqdm(test_loader, desc=\"Prediksi Test\"):\n",
    "        image = image[0] # Hapus dimensi batch\n",
    "        filename = filename[0]\n",
    "        \n",
    "        ensemble_probs = torch.zeros(1, num_classes).to(device)\n",
    "        \n",
    "        for model in ensemble_models:\n",
    "            tta_probs = torch.zeros(1, num_classes).to(device)\n",
    "            for t_transform in tta_transforms:\n",
    "                transformed_image = t_transform(image).unsqueeze(0).to(device)\n",
    "                outputs = model(transformed_image)\n",
    "                tta_probs += nn.functional.softmax(outputs, dim=1)\n",
    "            ensemble_probs += (tta_probs / len(tta_transforms))\n",
    "            \n",
    "        final_probs = ensemble_probs / len(ensemble_models)\n",
    "        _, predicted_idx = final_probs.max(1)\n",
    "        \n",
    "        predictions_indices.append(predicted_idx.cpu().item())\n",
    "        filenames.append(filename)\n",
    "\n",
    "# --- Buat file submission ---\n",
    "class_to_number = {'COVID': 0, 'Normal': 1, 'Viral Pneumonia': 2}\n",
    "final_predictions = [class_to_number.get(class_names[idx], -1) for idx in predictions_indices]\n",
    "filenames_no_ext = [f.replace('.png', '') for f in filenames]\n",
    "\n",
    "results_df = pd.DataFrame({'ID': filenames_no_ext, 'Predicted': final_predictions})\n",
    "submission_path = 'submission.csv'\n",
    "results_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Prediksi selesai! File disimpan di: {submission_path}\")\n",
    "print(\"\\nPreview 10 prediksi pertama:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "print(\"\\nDistribusi prediksi:\")\n",
    "print(results_df['Predicted'].value_counts(normalize=True).sort_index() * 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
