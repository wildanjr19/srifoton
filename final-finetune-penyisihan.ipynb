{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ba133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import densenet121\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import seaborn as sns\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# =============================================================================\n",
    "# 1) SEED & HELPER\n",
    "# =============================================================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Random seed diatur ke {SEED}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Menggunakan device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2) DATASET & TRANSFORM (mempertahankan teknik pemrosesan Anda)\n",
    "# =============================================================================\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if not is_test:\n",
    "            self.dataset = datasets.ImageFolder(root_dir)\n",
    "            self.samples = self.dataset.samples\n",
    "            self.classes = self.dataset.classes\n",
    "        else:\n",
    "            self.image_paths = []\n",
    "            for file in sorted(os.listdir(root_dir)):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(root_dir, file))\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.is_test:\n",
    "            return len(self.image_paths)\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.image_paths[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, os.path.basename(img_path)\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "class ApplyCLAHE(object):\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_np = np.array(img)\n",
    "        if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n",
    "            lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            l_clahe = self.clahe.apply(l)\n",
    "            lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "            img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2RGB)\n",
    "            return Image.fromarray(img_clahe)\n",
    "        else:\n",
    "            gray_clahe = self.clahe.apply(img_np)\n",
    "            return Image.fromarray(gray_clahe)\n",
    "\n",
    "# =============================================================================\n",
    "# 3) KONFIGURASI\n",
    "# =============================================================================\n",
    "IMG_SIZE = (384, 384)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_FOLDS = 4\n",
    "\n",
    "# Path data & bobot model 5-kelas lama\n",
    "TRAIN_PATH = \"/kaggle/input/final-srifoton-25-machine-learning-competition/train/train\"  # sekarang berisi 3 kelas\n",
    "TEST_PATH  = \"/kaggle/input/final-srifoton-25-machine-learning-competition/test/test\"\n",
    "PREV_5CLS_CHECKPOINT = \"/kaggle/input/weights/model_5cls.pth\"  # GANTI dengan path bobot lama Anda\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=10,\n",
    "        translate=(0.05, 0.05),\n",
    "        scale=(0.95, 1.05),\n",
    "        shear=(-5, 5, -5, 5),\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        fill=0),\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Memuat dataset...\")\n",
    "full_train_dataset = ImageDataset(TRAIN_PATH, transform=None)\n",
    "class_names = full_train_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Kelas: {class_names} (n={num_classes})\")\n",
    "\n",
    "samples = full_train_dataset.samples\n",
    "paths, labels = zip(*samples)\n",
    "labels = np.array(labels)\n",
    "\n",
    "test_dataset = ImageDataset(TEST_PATH, transform=val_test_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4) MEMBANGUN MODEL DARI BOOTSTRAP 5-KELAS → GANTI HEAD → 3-KELAS\n",
    "# =============================================================================\n",
    "def load_any_checkpoint(ckpt_path, map_location='cpu'):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'state_dict' in ckpt:\n",
    "            return ckpt['state_dict']\n",
    "        elif 'model' in ckpt:\n",
    "            return ckpt['model']\n",
    "        else:\n",
    "            return ckpt\n",
    "    return ckpt\n",
    "\n",
    "def build_backbone_from_checkpoint(ckpt_path, num_classes_new):\n",
    "    \"\"\"\n",
    "    Asumsi backbone lama DenseNet121. \n",
    "    - Load state_dict lama (5 kelas).\n",
    "    - Salin weight 'features' ke backbone baru.\n",
    "    - Ganti classifier → num_classes_new.\n",
    "    \"\"\"\n",
    "    # 1) Siapkan model backbone baru\n",
    "    model = densenet121(pretrained=True)\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features, num_classes_new)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 2) Muat state_dict lama dan mapping\n",
    "    try:\n",
    "        old_sd = load_any_checkpoint(ckpt_path, map_location='cpu')\n",
    "\n",
    "        # Hilangkan prefix 'module.' jika ada (DDP)\n",
    "        new_sd = {}\n",
    "        for k, v in old_sd.items():\n",
    "            name = k[7:] if k.startswith('module.') else k\n",
    "            new_sd[name] = v\n",
    "\n",
    "        # 3) Ambil hanya weight 'features' (abaikan classifier lama 5 kelas)\n",
    "        model_sd = model.state_dict()\n",
    "        transferable = {k: v for k, v in new_sd.items() if k.startswith('features.') and k in model_sd}\n",
    "        model_sd.update(transferable)\n",
    "        model.load_state_dict(model_sd, strict=False)\n",
    "        print(f\"Backbone terinisialisasi dari checkpoint 5-kelas: {ckpt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"PERINGATAN: Gagal memuat backbone dari checkpoint: {e}\")\n",
    "        print(\"Model akan mulai dari ImageNet pretrained.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Wrapper untuk training\n",
    "class MyBackboneFineTune(nn.Module):\n",
    "    def __init__(self, num_classes, prev_ckpt):\n",
    "        super().__init__()\n",
    "        self.model = build_backbone_from_checkpoint(prev_ckpt, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# =============================================================================\n",
    "# 5) TRAINING UTILS\n",
    "# =============================================================================\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# =============================================================================\n",
    "# 6) K-FOLD FINE-TUNING (dari backbone 5-kelas → 3-kelas)\n",
    "# =============================================================================\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "model_paths, fold_val_accs, fold_val_f1s = [], [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(paths, labels)):\n",
    "    print(f\"\\n=== Fold {fold+1}/{NUM_FOLDS} ===\")\n",
    "\n",
    "    # Subset\n",
    "    train_subset = Subset(ImageDataset(TRAIN_PATH, transform=train_transform), train_idx)\n",
    "    val_subset   = Subset(ImageDataset(TRAIN_PATH, transform=val_test_transform), val_idx)\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "    val_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Model dari bobot 5-kelas → head 3-kelas\n",
    "    model = MyBackboneFineTune(num_classes=num_classes, prev_ckpt=PREV_5CLS_CHECKPOINT).to(device)\n",
    "\n",
    "    # Stage 1: freeze fitur, latih classifier (warm-up)\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'classifier' in n:\n",
    "            p.requires_grad = True\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    opt_head = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "    epochs_head = 5\n",
    "    for ep in range(epochs_head):\n",
    "        print(f\"[Fold {fold+1}] Head Epoch {ep+1}/{epochs_head}\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_epoch(model, train_loader, criterion, opt_head, device)\n",
    "        va_loss, va_acc, va_f1 = validate_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Head Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% f1={tr_f1:.4f} | \"\n",
    "              f\"Val: loss={va_loss:.4f} acc={va_acc:.2f}% f1={va_f1:.4f}\")\n",
    "\n",
    "    # Stage 2: unfreeze semua, fine-tune halus\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    opt_full = optim.AdamW(model.parameters(), lr=LEARNING_RATE/10, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt_full, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    train_losses, train_accs, train_f1s = [], [], []\n",
    "    val_losses, val_accs, val_f1s = [], [], []\n",
    "    best_val_acc = 0.0\n",
    "    best_model_path = f'best_model_fold{fold+1}.pth'\n",
    "    patience, patience_counter = 7, 0\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        print(f\"[Fold {fold+1}] FT Epoch {ep+1}/{EPOCHS}\")\n",
    "        tr_loss, tr_acc, tr_f1 = train_epoch(model, train_loader, criterion, opt_full, device)\n",
    "        va_loss, va_acc, va_f1 = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(tr_loss); train_accs.append(tr_acc); train_f1s.append(tr_f1)\n",
    "        val_losses.append(va_loss);   val_accs.append(va_acc);   val_f1s.append(va_f1)\n",
    "\n",
    "        scheduler.step(va_acc)\n",
    "        print(f\"Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% f1={tr_f1:.4f} | \"\n",
    "              f\"Val: loss={va_loss:.4f} acc={va_acc:.2f}% f1={va_f1:.4f}\")\n",
    "\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            patience_counter = 0\n",
    "            print(f\"** Best Val Acc updated: {best_val_acc:.2f}% (model disimpan: {best_model_path})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\">> Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model_paths.append(best_model_path)\n",
    "    fold_val_accs.append(best_val_acc)\n",
    "    fold_val_f1s.append(max(val_f1s))\n",
    "\n",
    "# Ringkasan CV\n",
    "print(\"\\n=== Hasil Cross-Validation ===\")\n",
    "print(f\"Rata-rata Val Acc: {np.mean(fold_val_accs):.2f}%\")\n",
    "print(f\"Rata-rata Val F1 : {np.mean(fold_val_f1s):.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7) MUAT MODEL TERBAIK PER FOLD UNTUK EVALUASI & ENSEMBLE\n",
    "# =============================================================================\n",
    "def load_finetuned_model(path_ckpt, num_classes):\n",
    "    m = MyBackboneFineTune(num_classes=num_classes, prev_ckpt=PREV_5CLS_CHECKPOINT).to(device)\n",
    "    # Muat head yang sudah fine-tuned 3 kelas\n",
    "    sd = torch.load(path_ckpt, map_location='cpu')\n",
    "    m.load_state_dict(sd, strict=True)\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "models = [load_finetuned_model(p, num_classes) for p in model_paths]\n",
    "\n",
    "# (Opsional) Evaluasi Confusion Matrix dengan val_loader dari fold terakhir\n",
    "print(\"\\nEvaluasi (last fold) pada validation set...\")\n",
    "last_fold_val_idx = list(skf.split(paths, labels))[-1][1]\n",
    "val_subset_last = Subset(ImageDataset(TRAIN_PATH, transform=val_test_transform), last_fold_val_idx)\n",
    "val_loader_last = DataLoader(val_subset_last, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader_last, desc=\"Validating\"):\n",
    "        images = images.to(device)\n",
    "        # rata-rata ensemble prediksi logits → softmax\n",
    "        logits_sum = None\n",
    "        for m in models:\n",
    "            out = m(images)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "        probs = torch.softmax(logits_sum / len(models), dim=1)\n",
    "        pred = probs.argmax(1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Classification Report (Last Fold, Ensemble)\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Last Fold, Ensemble)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8) PREDIKSI TEST DENGAN TTA + ENSEMBLE\n",
    "# =============================================================================\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "hflip_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.Lambda(lambda im: F.hflip(im)),  # selalu flip\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "rotate_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    ApplyCLAHE(clip_limit=2.0),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tta_transforms = [base_transform, hflip_transform, rotate_transform]\n",
    "print(f\"\\nMenggunakan {len(tta_transforms)} transformasi untuk TTA per model.\")\n",
    "\n",
    "# Dapatkan daftar file test\n",
    "test_image_paths = sorted([os.path.join(TEST_PATH, f)\n",
    "                           for f in os.listdir(TEST_PATH)\n",
    "                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "print(f\"Menemukan {len(test_image_paths)} gambar test.\")\n",
    "\n",
    "predictions_indices, filenames = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image_path in tqdm(test_image_paths, desc=\"Testing with TTA + Ensemble\"):\n",
    "        fn = os.path.basename(image_path)\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Ensemble akumulasi probabilitas\n",
    "        ensemble_probs = torch.zeros(1, num_classes).to(device)\n",
    "\n",
    "        for m in models:\n",
    "            tta_probs = torch.zeros(1, num_classes).to(device)\n",
    "            for tta in tta_transforms:\n",
    "                x = tta(img).unsqueeze(0).to(device)\n",
    "                out = m(x)\n",
    "                probs = torch.softmax(out, dim=1)\n",
    "                tta_probs += probs\n",
    "            ensemble_probs += (tta_probs / len(tta_transforms))\n",
    "\n",
    "        avg_probs = ensemble_probs / len(models)\n",
    "        pred_idx = avg_probs.argmax(1).item()\n",
    "\n",
    "        predictions_indices.append(pred_idx)\n",
    "        filenames.append(fn)\n",
    "\n",
    "# Mapping kelas → angka mengikuti urutan class_names\n",
    "class_to_number = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "final_numbers = [pred_idx for pred_idx in predictions_indices]\n",
    "filenames_no_ext = [os.path.splitext(f)[0] for f in filenames]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': filenames_no_ext,\n",
    "    'Predicted': final_numbers\n",
    "})\n",
    "submission_path = 'submission.csv'\n",
    "results_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Prediksi selesai! File disimpan di: {submission_path}\")\n",
    "print(\"Preview 10 baris:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "print(\"\\nDistribusi prediksi:\")\n",
    "counts = results_df['Predicted'].value_counts().sort_index()\n",
    "percentages = results_df['Predicted'].value_counts(normalize=True).sort_index() * 100\n",
    "distribution = pd.DataFrame({'Count': counts, 'Percentage': percentages.round(2)})\n",
    "print(distribution)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
