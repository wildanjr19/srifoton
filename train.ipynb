{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths - sesuaikan dengan struktur folder Anda\n",
    "TRAIN_PATH = \"path/to/train\"  # Ganti dengan path train folder Anda\n",
    "VAL_PATH = \"path/to/val\"      # Ganti dengan path val folder Anda  \n",
    "TEST_PATH = \"path/to/test\"    # Ganti dengan path test folder Anda\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "NUM_CLASSES = 5\n",
    "PATIENCE = 10  # Early stopping patience\n",
    "IMG_SIZE = 224\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "        \n",
    "        if is_test:\n",
    "            # Untuk test set, ambil semua file gambar\n",
    "            for file_name in os.listdir(root_dir):\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(root_dir, file_name), file_name))\n",
    "        else:\n",
    "            # Untuk train/val set, ambil berdasarkan subfolder kelas\n",
    "            self.class_to_idx = {}\n",
    "            idx = 0\n",
    "            \n",
    "            for class_name in sorted(os.listdir(root_dir)):\n",
    "                class_path = os.path.join(root_dir, class_name)\n",
    "                if os.path.isdir(class_path):\n",
    "                    self.class_to_idx[class_name] = idx\n",
    "                    \n",
    "                    for file_name in os.listdir(class_path):\n",
    "                        if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            file_path = os.path.join(class_path, file_name)\n",
    "                            self.samples.append((file_path, idx))\n",
    "                    idx += 1\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path, file_name = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, file_name\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "\n",
    "# Data transforms dengan augmentasi untuk training\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = CustomDataset(TRAIN_PATH, transform=train_transforms, is_test=False)\n",
    "val_dataset = CustomDataset(VAL_PATH, transform=val_transforms, is_test=False)\n",
    "test_dataset = CustomDataset(TEST_PATH, transform=test_transforms, is_test=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# DINOv2 Model Class\n",
    "class DINOv2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=5, model_name='dinov2_vits14'):\n",
    "        super(DINOv2Classifier, self).__init__()\n",
    "        \n",
    "        # Load DINOv2 model\n",
    "        self.backbone = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        \n",
    "        # Freeze backbone parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Get feature dimension\n",
    "        if 'vits14' in model_name:\n",
    "            feature_dim = 384\n",
    "        elif 'vitb14' in model_name:\n",
    "            feature_dim = 768\n",
    "        elif 'vitl14' in model_name:\n",
    "            feature_dim = 1024\n",
    "        elif 'vitg14' in model_name:\n",
    "            feature_dim = 1536\n",
    "        else:\n",
    "            feature_dim = 384  # default\n",
    "            \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from DINOv2\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(x)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "print(\"Loading DINOv2 model...\")\n",
    "model = DINOv2Classifier(num_classes=NUM_CLASSES, model_name='dinov2_vits14')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc='Training')\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(train_bar.n+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc='Validation')\n",
    "        for images, labels in val_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(val_bar.n+1):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return running_loss / len(val_loader), 100. * correct / total\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_dinov2_lung_classifier.pth')\n",
    "        print(f'New best model saved with validation accuracy: {best_val_acc:.2f}%')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "\n",
    "# Load best model for testing\n",
    "print(\"\\nLoading best model for testing...\")\n",
    "model.load_state_dict(torch.load('best_dinov2_lung_classifier.pth'))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Final validation evaluation\n",
    "print(\"\\nFinal validation evaluation...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc='Final Validation'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Test Time Augmentation transforms\n",
    "tta_transforms = [\n",
    "    # Original\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Horizontal flip\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Rotation +10 degrees\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomRotation(degrees=(10, 10)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Rotation -10 degrees\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomRotation(degrees=(-10, -10)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Brightness adjustment\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ColorJitter(brightness=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Scale variation\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((int(IMG_SIZE * 1.1), int(IMG_SIZE * 1.1))),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "]\n",
    "\n",
    "def predict_with_tta(model, image_path, transforms_list, device):\n",
    "    \"\"\"\n",
    "    Perform prediction with Test Time Augmentation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for transform in transforms_list:\n",
    "            # Load and transform image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            output = model(image_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            predictions.append(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Average predictions across all augmentations\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    predicted_class = np.argmax(avg_predictions)\n",
    "    confidence = np.max(avg_predictions)\n",
    "    \n",
    "    return predicted_class, confidence, avg_predictions[0]\n",
    "\n",
    "# Test predictions with TTA\n",
    "print(\"\\nGenerating test predictions with Test Time Augmentation...\")\n",
    "print(\"Using TTA with following augmentations:\")\n",
    "print(\"- Original image\")\n",
    "print(\"- Horizontal flip\") \n",
    "print(\"- Rotation +10°\")\n",
    "print(\"- Rotation -10°\")\n",
    "print(\"- Brightness adjustment\")\n",
    "print(\"- Scale variation\")\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "filenames = []\n",
    "confidences = []\n",
    "all_probabilities = []\n",
    "\n",
    "# Get list of test files\n",
    "test_files = []\n",
    "for file_name in os.listdir(TEST_PATH):\n",
    "    if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        test_files.append(file_name)\n",
    "\n",
    "test_files = sorted(test_files)\n",
    "\n",
    "# Process each test file with TTA\n",
    "for file_name in tqdm(test_files, desc='TTA Prediction'):\n",
    "    file_path = os.path.join(TEST_PATH, file_name)\n",
    "    \n",
    "    # Get TTA prediction\n",
    "    predicted_class, confidence, probabilities = predict_with_tta(\n",
    "        model, file_path, tta_transforms, device\n",
    "    )\n",
    "    \n",
    "    predictions.append(predicted_class)\n",
    "    filenames.append(file_name)\n",
    "    confidences.append(confidence)\n",
    "    all_probabilities.append(probabilities)\n",
    "\n",
    "# Create submission DataFrame with confidence scores\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': filenames,\n",
    "    'Predicted': predictions,\n",
    "    'Confidence': confidences\n",
    "})\n",
    "\n",
    "# Create detailed probabilities DataFrame (optional, for analysis)\n",
    "prob_columns = [f'Prob_Class_{i}' for i in range(NUM_CLASSES)]\n",
    "probabilities_df = pd.DataFrame(all_probabilities, columns=prob_columns)\n",
    "probabilities_df['Id'] = filenames\n",
    "probabilities_df['Predicted'] = predictions\n",
    "\n",
    "# Sort by Id for consistency\n",
    "submission_df = submission_df.sort_values('Id').reset_index(drop=True)\n",
    "probabilities_df = probabilities_df.sort_values('Id').reset_index(drop=True)\n",
    "\n",
    "# Save main submission file (Kaggle format)\n",
    "kaggle_submission = submission_df[['Id', 'Predicted']].copy()\n",
    "kaggle_submission.to_csv('submission_tta.csv', index=False)\n",
    "\n",
    "# Save detailed results with confidence scores\n",
    "submission_df.to_csv('submission_tta_detailed.csv', index=False)\n",
    "\n",
    "# Save probabilities for further analysis\n",
    "probabilities_df.to_csv('test_probabilities_tta.csv', index=False)\n",
    "\n",
    "print(f\"\\nTTA Submission files saved:\")\n",
    "print(f\"- submission_tta.csv (Kaggle format)\")\n",
    "print(f\"- submission_tta_detailed.csv (with confidence scores)\")\n",
    "print(f\"- test_probabilities_tta.csv (with all class probabilities)\")\n",
    "print(f\"Total test predictions: {len(submission_df)}\")\n",
    "print(f\"Average confidence: {np.mean(confidences):.4f}\")\n",
    "print(f\"Prediction distribution:\")\n",
    "print(submission_df['Predicted'].value_counts().sort_index())\n",
    "\n",
    "# Display confidence statistics\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"Min confidence: {np.min(confidences):.4f}\")\n",
    "print(f\"Max confidence: {np.max(confidences):.4f}\")\n",
    "print(f\"Mean confidence: {np.mean(confidences):.4f}\")\n",
    "print(f\"Std confidence: {np.std(confidences):.4f}\")\n",
    "\n",
    "# Show low confidence predictions (might need manual review)\n",
    "low_conf_threshold = np.percentile(confidences, 10)  # Bottom 10%\n",
    "low_conf_predictions = submission_df[submission_df['Confidence'] < low_conf_threshold]\n",
    "print(f\"\\nLow confidence predictions (< {low_conf_threshold:.4f}):\")\n",
    "print(f\"Count: {len(low_conf_predictions)}\")\n",
    "if len(low_conf_predictions) > 0:\n",
    "    print(low_conf_predictions.head())\n",
    "\n",
    "print(\"\\nTraining and TTA prediction completed successfully!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
